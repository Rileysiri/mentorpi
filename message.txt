#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
import time, threading, math
import cv2
import mediapipe as mp
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from ros_robot_controller_msgs.msg import BuzzerState

class RobotDancer(Node):
    def __init__(self):
        super().__init__('robot_dancer')
        self.publisher_ = self.create_publisher(Twist, 'cmd_vel', 10)
        self.buzzer_pub = self.create_publisher(BuzzerState, '/ros_robot_controller/set_buzzer', 1)
        
        # CV Bridge and frame storage.
        self.bridge = CvBridge()
        self.frame = None

        # State flags.
        self.face_detected = False
        self.current_gesture = None
        self.busy_dancing_30s = False  # Also used to lock out new commands during a spin or dance

        # Debug flag.
        self.debug = True

        # -------------------------
        # MediaPipe Hands.
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(min_detection_confidence=0.7,
                                         min_tracking_confidence=0.7)
        self.mp_draw = mp.solutions.drawing_utils

        # -------------------------
        # Face detection (Haar Cascade).
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.scale_factor = 1.1
        self.min_neighbors = 8
        self.min_size = (100, 100)

        # Subscribe to the camera image topic.
        self.create_subscription(Image, 'ascamera/camera_publisher/rgb0/image',
                                 self.image_callback, 1)

    def image_callback(self, msg):
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.frame = cv_image
            if self.debug:
                self.get_logger().info("Image received in callback.")
                print("DEBUG: Image received in callback.")
        except Exception as e:
            self.get_logger().error("Failed to convert image: " + str(e))
            print("DEBUG: Error in image callback:", e)

    ##################################################
    # Basic Movements
    ##################################################
    def move_robot(self, x, y, z, duration):
        msg = Twist()
        msg.linear.x = float(x)
        msg.linear.y = float(y)
        msg.angular.z = float(z)
        start_time = time.time()
        while (time.time() - start_time < duration) and rclpy.ok():
            self.publisher_.publish(msg)
            time.sleep(0.05)
        self.stop_robot()

    def stop_robot(self):
        msg = Twist()
        self.publisher_.publish(msg)

    ##################################################
    # Happy Dance (triggered by face detection)
    ##################################################
    def cute_chirp(self):
        chirp_notes = [
            (800, 0.2, 0.05),
            (1000, 0.2, 0.05)
        ]
        for freq, on_time, off_time in chirp_notes:
            msg = BuzzerState()
            msg.freq = freq
            msg.on_time = on_time
            msg.off_time = off_time
            msg.repeat = 1
            self.buzzer_pub.publish(msg)
            time.sleep(on_time + off_time)

    def happy_dance(self):
        self.get_logger().info("Performing HAPPY dance for face recognition!")
        print("DEBUG: Face recognized => HAPPY dance triggered!")
        self.cute_chirp()
        # Perform two fast forward/back cycles.
        for _ in range(2):
            self.move_robot(0.3, 0.0, 0.0, 0.3)  # forward
            self.move_robot(-0.3, 0.0, 0.0, 0.3) # back

    ##################################################
    # Dedicated Spin Routine (for 3-finger gesture)
    ##################################################
    def spin_robot(self):
        self.get_logger().info("Starting full 360° spin!")
        print("DEBUG: Spin initiated – committing to full 360° spin.")
        self.busy_dancing_30s = True
        # Command a full spin (adjust duration/rotation as needed).
        self.move_robot(0.0, 0.0, 3.14, 13.5)
        self.busy_dancing_30s = False

    ##################################################
    # 30-Second Song & Dance Routine (for 2-finger gesture)
    ##################################################
    def play_note_and_move(self, freq, on_time, off_time, x, y, z):
        total_duration = on_time + off_time
        msg = BuzzerState()
        msg.freq = freq
        msg.on_time = on_time
        msg.off_time = off_time
        msg.repeat = 1
        self.buzzer_pub.publish(msg)
        step_start = time.time()
        while (time.time() - step_start < total_duration) and rclpy.ok():
            # If a stop command comes in during the routine, abort.
            if self.current_gesture == "stop":
                self.stop_robot()
                return "STOPPED"
            tmsg = Twist()
            tmsg.linear.x = float(x)
            tmsg.linear.y = float(y)
            tmsg.angular.z = float(z)
            self.publisher_.publish(tmsg)
            time.sleep(0.05)
        self.stop_robot()
        return "OK"

    def perform_30s_song_and_dance(self):
        self.busy_dancing_30s = True
        self.get_logger().info("Starting 30-second choreographed dance routine!")
        print("DEBUG: 30s dance routine triggered.")
        # (For brevity, the dance routine phases remain unchanged.)
        slow_notes = [
            (220, 1.5, 0.5),
            (261, 1.5, 0.5),
            (293, 1.5, 0.5),
            (329, 1.5, 0.5),
            (349, 1.5, 0.5),
        ]
        slow_moves = [
            (0.1, 0.0, 0.2),
            (0.1, 0.1, 0.0),
            (0.0, 0.0, 0.3),
            (0.1, -0.1, 0.1),
            (0.2, 0.0, 0.2),
        ]
        for i in range(len(slow_notes)):
            freq, on_time, off_time = slow_notes[i]
            x, y, z = slow_moves[i]
            status = self.play_note_and_move(freq, on_time, off_time, x, y, z)
            if status == "STOPPED":
                self.get_logger().info("Dance interrupted by stop gesture.")
                print("DEBUG: Stop gesture during dance; routine aborted.")
                self.busy_dancing_30s = False
                return

        medium_notes = [
            (392, 0.8, 0.2),
            (440, 0.8, 0.2),
            (494, 0.8, 0.2),
            (523, 0.8, 0.2),
            (587, 0.8, 0.2),
            (523, 0.8, 0.2),
            (494, 0.8, 0.2),
            (440, 0.8, 0.2),
            (392, 0.8, 0.2),
            (370, 0.8, 0.2),
        ]
        medium_moves = [
            (0.3, 0.0, 0.0),
            (0.0, 0.3, 0.0),
            (-0.3, 0.0, 0.0),
            (0.0, -0.3, 0.0),
            (0.0, 0.0, 0.8),
            (0.2, 0.2, 0.0),
            (0.0, 0.0, -0.8),
            (0.3, 0.0, 0.0),
            (0.0, -0.3, 0.2),
            (0.0, 0.3, -0.2),
        ]
        for i in range(len(medium_notes)):
            freq, on_time, off_time = medium_notes[i]
            x, y, z = medium_moves[i]
            status = self.play_note_and_move(freq, on_time, off_time, x, y, z)
            if status == "STOPPED":
                self.get_logger().info("Dance interrupted by stop gesture.")
                print("DEBUG: Stop gesture during dance; routine aborted.")
                self.busy_dancing_30s = False
                return

        fast_notes = []
        freq_start = 600
        for i in range(20):
            freq = freq_start + i * 20
            fast_notes.append((freq, 0.4, 0.1))
        fast_moves = []
        for i in range(20):
            if i % 2 == 0:
                fast_moves.append((0.0, 0.0, 2.0))
            else:
                fast_moves.append((0.4, 0.0, 0.0))
        for i in range(len(fast_notes)):
            freq, on_time, off_time = fast_notes[i]
            x, y, z = fast_moves[i]
            status = self.play_note_and_move(freq, on_time, off_time, x, y, z)
            if status == "STOPPED":
                self.get_logger().info("Dance interrupted by stop gesture.")
                print("DEBUG: Stop gesture during dance; routine aborted.")
                self.busy_dancing_30s = False
                return

        self.get_logger().info("Completed the 30-second dance routine!")
        print("DEBUG: 30s dance done!")
        self.busy_dancing_30s = False

    ##################################################
    # Gesture Recognition
    ##################################################
    def get_index_extension_ratio(self, hand_landmarks):
        """Compute a normalized ratio for index finger extension."""
        pip = hand_landmarks.landmark[6]
        tip = hand_landmarks.landmark[8]
        wrist = hand_landmarks.landmark[0]
        middle_tip = hand_landmarks.landmark[12]
        hand_size = math.sqrt((middle_tip.x - wrist.x) ** 2 + (middle_tip.y - wrist.y) ** 2)
        dist = math.sqrt((tip.x - pip.x) ** 2 + (tip.y - pip.y) ** 2)
        return dist / hand_size

    def interpret_hand_gesture(self, hand_landmarks):
        # For each finger (ignoring thumb), decide if it's extended by comparing its tip and pip y-coordinates.
        index_tip_y  = hand_landmarks.landmark[8].y
        index_pip_y  = hand_landmarks.landmark[6].y
        middle_tip_y = hand_landmarks.landmark[12].y
        middle_pip_y = hand_landmarks.landmark[10].y
        ring_tip_y   = hand_landmarks.landmark[16].y
        ring_pip_y   = hand_landmarks.landmark[14].y
        pinky_tip_y  = hand_landmarks.landmark[20].y
        pinky_pip_y  = hand_landmarks.landmark[18].y

        # Use a fixed margin to decide if a finger is extended.
        index_extended  = (index_tip_y < index_pip_y - 0.03)
        middle_extended = (middle_tip_y < middle_pip_y - 0.03)
        ring_extended   = (ring_tip_y < ring_pip_y - 0.03)
        pinky_extended  = (pinky_tip_y < pinky_pip_y - 0.03)

        # Count the number of extended fingers.
        count_extended = sum([index_extended, middle_extended, ring_extended, pinky_extended])

        # Compute the index extension ratio.
        index_ratio = self.get_index_extension_ratio(hand_landmarks)
        
        if self.debug:
            debug_msg = (f"Index: {'extended' if index_extended else ('bent' if (0.20 < index_ratio < 0.33) else 'closed')}, "
                         f"Middle: {middle_extended}, Ring: {ring_extended}, Pinky: {pinky_extended}, "
                         f"Count: {count_extended}, Index Ratio: {index_ratio:.3f}")
            self.get_logger().info(debug_msg)
            print("DEBUG:", debug_msg)

        # Mapping:
        # 0 extended fingers:
        #   If the index ratio is between 0.20 and 0.33, consider it bent → go_back;
        #   otherwise, if fully closed, → stop.
        if count_extended == 0:
            if 0.20 < index_ratio < 0.33:
                return "go_back"
            else:
                return "stop"
        # 1 extended finger: (only index extended) → come_forward.
        elif count_extended == 1:
            return "come_forward"
        # 2 extended fingers: → dance routine.
        elif count_extended == 2:
            return "dance_routine"
        # 3 extended fingers: → spin.
        elif count_extended == 3:
            return "spin"
        # 4 extended (open palm): → no action.
        elif count_extended == 4:
            return None
        else:
            return None

    ##################################################
    # Frame Processing / Subscription Loop
    ##################################################
    def sensor_listener(self):
        # Wait for the first frame.
        while self.frame is None and rclpy.ok():
            if self.debug:
                self.get_logger().info("Waiting for first frame...")
                print("DEBUG: Waiting for first frame...")
            time.sleep(0.1)
        while rclpy.ok():
            try:
                if self.frame is None:
                    if self.debug:
                        self.get_logger().info("No frame available yet.")
                        print("DEBUG: No frame available yet.")
                    time.sleep(0.05)
                    continue

                # Resize the frame to 320x240 for faster processing.
                frame = cv2.resize(self.frame, (320, 240))
                # Mirror the frame.
                frame = cv2.flip(frame, 1)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # Face detection.
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = self.face_cascade.detectMultiScale(
                    gray,
                    scaleFactor=self.scale_factor,
                    minNeighbors=self.min_neighbors,
                    minSize=self.min_size
                )
                self.face_detected = len(faces) > 0
                if self.face_detected and self.debug:
                    msg = f"Face detected: {len(faces)} face(s) found."
                    self.get_logger().info(msg)
                    print("DEBUG:", msg)
                for (x, y, w, h) in faces:
                    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)

                # Hand gesture detection.
                try:
                    results = self.hands.process(rgb_frame)
                    if results is not None and hasattr(results, "multi_hand_landmarks") and results.multi_hand_landmarks:
                        hand_landmarks = results.multi_hand_landmarks[0]
                        self.mp_draw.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                        gesture = self.interpret_hand_gesture(hand_landmarks)
                        self.current_gesture = gesture
                        if self.debug and gesture:
                            self.get_logger().info(f"Detected gesture: {gesture}")
                            print("DEBUG: Detected gesture:", gesture)
                    else:
                        self.current_gesture = None
                except Exception as e:
                    self.get_logger().error("Error processing hand landmarks: " + str(e))
                    print("DEBUG: Error processing hand landmarks:", e)
                    self.current_gesture = None

                cv2.imshow("Talent Show - Sensor Feed", frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            except Exception as ex:
                self.get_logger().error("Error in sensor_listener loop: " + str(ex))
                print("DEBUG: Error in sensor_listener loop:", ex)
            time.sleep(0.05)
        cv2.destroyAllWindows()

    ##################################################
    # Main Loop
    ##################################################
    def run_talent_show(self):
        self.get_logger().info("Talent show started. Face detection is conservative; hand gestures are re-mapped.")
        print("DEBUG: Gesture mapping:\n"
              "  Fist -> stop,\n"
              "  Bent index (with index ratio between 0.20 and 0.33) -> go_back,\n"
              "  1 finger -> come_forward,\n"
              "  2 fingers -> dance routine,\n"
              "  3 fingers -> spin (full 360°),\n"
              "  4 fingers -> no action.")
        while rclpy.ok():
            # Face detection triggers the happy dance.
            if self.face_detected and not self.busy_dancing_30s:
                self.happy_dance()
                time.sleep(1.0)

            gesture = self.current_gesture
            # Check and execute gesture commands only if not busy.
            if not self.busy_dancing_30s:
                if gesture == "dance_routine":
                    self.perform_30s_song_and_dance()
                elif gesture == "stop":
                    self.stop_robot()
                elif gesture == "come_forward":
                    self.move_robot(0.3, 0.0, 0.0, 1.0)
                elif gesture == "go_back":
                    self.move_robot(-0.3, 0.0, 0.0, 1.0)
                elif gesture == "spin":
                    self.spin_robot()
                else:
                    self.stop_robot()
            time.sleep(0.05)

def main(args=None):
    rclpy.init(args=args)
    dancer = RobotDancer()

    spin_thread = threading.Thread(target=rclpy.spin, args=(dancer,))
    spin_thread.start()

    sensor_thread = threading.Thread(target=dancer.sensor_listener)
    sensor_thread.start()

    dancer.get_logger().info("Sensor listener thread started.")
    print("DEBUG: Sensor listener thread started.")

    try:
        dancer.run_talent_show()
    except KeyboardInterrupt:
        dancer.get_logger().info("Talent show interrupted.")
        print("DEBUG: Talent show interrupted.")
    dancer.destroy_node()
    rclpy.shutdown()
    sensor_thread.join()
    spin_thread.join()

if __name__ == '__main__':
    main()

the happy dance should be a quick front and back twice